<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2026-01-16T04:17:32+00:00</updated><id>/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Open Positions (ENG)</title><link href="/blog/2023/mpil-info-eng/" rel="alternate" type="text/html" title="Open Positions (ENG)" /><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>/blog/2023/mpil-info-eng</id><content type="html" xml:base="/blog/2023/mpil-info-eng/"><![CDATA[<p>This article covers information that will help you join our lab!
If you are interested in our lab, please refer to the article below and contact me via e-mail.
Questions beyond the information provided in this article are always welcome.</p>

<hr />

<h2 id="number-of-openings">Number of Openings</h2>
<p>Our lab recruits students for Ph.D./Integrated/Master’s programs.
However, there are not many openings for the Master’s program, and only those who are expected to achieve meaningful research outcomes during the Master’s program are recruited.
This is to let M.S. students gain meaningful research experience in our laboratory and it is also helpful for their careers.</p>

<p>*For undergraduate research interns, the number of openings is flexible (see below).<br />
*For postdoctoral researcher positions, please contact me with your CV.<br /></p>

<hr />

<h2 id="graduate-programs">Graduate Programs</h2>
<p>If you are thinking of proceeding to graduate school, we recommend that you apply as an intern at least six months before the graduate school admission process.
It is important to carefully select a laboratory because graduate programs can take as short as two years and as long as five years or more. Below is a description of each program.</p>

<blockquote>
  <p>Ph.D./Integrated Program</p>
  <ul>
    <li>
      <h6 id="i-only-accept-students-who-have-prior-research-experience-with-me-or-who-have-received-reliable-recommendations">I only accept students who have prior research experience with me or who have received reliable recommendations.</h6>
    </li>
    <li>
      <h6 id="if-you-do-not-have-research-experience-with-me-or-have-not-received-a-recommendation-please-apply-for-an-intern-or-the-masters-program">If you do not have research experience with me or have not received a recommendation, please apply for an intern or the Master’s program.</h6>
    </li>
    <li>
      <h6 id="gist-ai-graduate-school-only-accepts-applicants-with-a-masters-degree-for-phd-programs">GIST AI graduate school only accepts applicants with a Master’s degree for Ph.D. programs.</h6>
    </li>
    <li>
      <h6 id="if-you-wish-to-obtain-only-a-masters-degree-you-must-apply-for-the-masters-program-and-the-integrated-program-is-only-for-students-who-wish-to-obtain-a-phd-degree">If you wish to obtain only a Master’s degree, you must apply for the Master’s program, and the integrated program is only for students who wish to obtain a Ph.D. degree.</h6>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p>Master’s Program</p>
  <ul>
    <li>
      <h6 id="in-depth-prior-research-experience-is-not-required-for-students-applying-for-the-masters-program">In-depth prior research experience is not required for students applying for the Master’s program.</h6>
    </li>
    <li>
      <h6 id="however-meaningful-prior-research-experience-is-a-powerful-advantage">However, meaningful prior research experience is a powerful advantage.</h6>
    </li>
    <li>
      <h6 id="this-is-the-recruitment-process-1-reviewing-application-documents-and-2-interviews-with-coding-tests">This is the recruitment process: 1) reviewing application documents and 2) interviews with coding tests.</h6>
    </li>
    <li>
      <h6 id="how-to-apply-please-email-me-your-answers-to-the-questions-below-and-your-cv-and-transcripts">[How to apply] Please email me your answers to the questions below and your CV and transcripts.</h6>
      <ul>
        <li>
          <h6 id="reasons-for-entering-the-masters-program-korean-or-english">Reasons for entering the Master’s program (Korean or English)</h6>
        </li>
        <li>
          <h6 id="your-career-plan-after-the-masters-program-korean-or-english">Your career plan after the Master’s program (Korean or English)</h6>
        </li>
        <li>
          <h6 id="reasons-for-applying-to-our-lab-korean-or-english">Reasons for applying to our lab (Korean or English)</h6>
        </li>
        <li>
          <h6 id="research-field-of-interest-and-related-experience-korean-or-english">Research field of interest and related experience (Korean or English)</h6>
        </li>
        <li>
          <h6 id="personal-statement-and-statement-of-purpose-these-include-answers-to-the-above-four-questions-english-300-words-or-less">Personal statement and statement of purpose; these include answers to the above four questions (English, 300 words or less)</h6>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr />

<h2 id="internship">Internship</h2>
<p>We recruit interns every semester and vacation, and intership is operated on a quarterly basis (January-February/March-June/July-August/September-December). We recommend a minimum of 6 months. In addition, our laboratory expects similar research outcomes for internship, graduation research, and G-SURF (you can combine internship and graduation research). If you are interested in our intern program, apply no later than two weeks before the beginning of each quarter. When applying, include transcripts and description of yourself. For reference, the following intern researchers are recruited with priority.</p>

<ul>
  <li>Students who want to explore new fields with intellectual curiosity</li>
  <li>Students who grow steadily by setting challenging goals</li>
  <li>Students who consistently achieve goals and finish what they have started</li>
  <li>Students with solid foundations in introductory-level programming skills (GS1401, GS1490)</li>
</ul>

<p>Below is how we run the internship program.</p>

<blockquote>
  <p>Study Intern</p>
  <ul>
    <li>
      <h6 id="through-study-study-interns-learn-basic-knowledge-for-conducting-research-in-our-laboratory">Through study, study interns learn basic knowledge for conducting research in our laboratory.</h6>
    </li>
    <li>
      <h6 id="a-mentor-in-charge-operates-the-study-study-materials-qa-etc">A mentor in charge operates the study (study materials, Q&amp;A, etc.).</h6>
    </li>
    <li>
      <h6 id="the-mentor-gives-study-interns-everyweeks-study-materials--assignments-and-checks-the-progress-and-takes-questions-through-weekly-meetings">The mentor gives study interns everyweek’s study materials / assignments, and checks the progress and takes questions through weekly meetings.</h6>
    </li>
    <li>
      <h6 id="the-study-has-three-courses-1-machine-learning-and-deep-learning-2-data-structures-and-algorithms-and-3-software-engineering">The study has three courses: 1) Machine Learning and Deep Learning, 2) Data Structures and Algorithms, and 3) Software Engineering.</h6>
    </li>
    <li>
      <h6 id="of-the-three-courses-you-will-participate-in-one-or-more-studies-depending-on-your-experience">Of the three courses, you will participate in one or more studies depending on your experience.</h6>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p>Research Intern</p>
  <ul>
    <li>
      <h6 id="research-interns-engage-in-a-research-project">Research interns engage in a research project.</h6>
    </li>
    <li>
      <h6 id="research-interns-usually-conduct-research-with-one-or-two-graduate-students-and-have-regular-research-meetings-with-me">Research interns usually conduct research with one or two graduate students, and have regular research meetings with me.</h6>
    </li>
    <li>
      <h6 id="one-can-apply-for-research-interns-only-if-they-have-completed-the-study-intern-course-or-have-background-knowledge-equivalent-to-it">One can apply for research interns only if they have completed the study intern course or have background knowledge equivalent to it.</h6>
    </li>
    <li>
      <h6 id="we-provide-research-interns-with-space-research-funds-and-relevant-devices">We provide research interns with space, research funds, and relevant devices.</h6>
    </li>
  </ul>
</blockquote>]]></content><author><name></name></author><summary type="html"><![CDATA[Information for Joining MPIL at GIST]]></summary></entry><entry><title type="html">Introduction to MPIL</title><link href="/blog/2023/mpil-info-kor/" rel="alternate" type="text/html" title="Introduction to MPIL" /><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>/blog/2023/mpil-info-kor</id><content type="html" xml:base="/blog/2023/mpil-info-kor/"><![CDATA[<h2 id="연구실-소개">연구실 소개</h2>

<ul>
  <li>저희 GIST MPIL 연구실에서는 스마트 모빌리티의 3차원 공간 인지 그리고 AR/VR의 공간 컴퓨팅을 위해 3D Geometric Data를 다루거나 생성하는 공간 인지 기술을 주로 연구합니다.</li>
  <li>본 연구실 홈페이지 Publications 탭에서 최근 진행중인 연구 및 논문들을 확인하실 수 있습니다.</li>
  <li>주로 Robotics, Computer Vision (CV), Machine Learning (ML) 분야의 top-tier 학회 및 저널에 논문을 publish하는 것을 목표로 합니다.
아래는 Robotics, CV, ML 분야에서 목표로 하는 학회들입니다. <br />
    <ul>
      <li>Robotics: ICRA, IROS, RSS, RA-L, IJRR, T-RO</li>
      <li>Computer Vision: CVPR, ECCV, ICCV, 3DV, T-PAMI, IJCV</li>
      <li>Machine Learning: NeurIPS, ICML, ICLR</li>
    </ul>
  </li>
  <li>3D Computer Vision 기반의 3차원 Geometric Data외에도 LiDAR, GNSS, IMU Data 등 더 다양하고 재미있는 연구 주제들을 다루는 것을 목표로 하고 있습니다.</li>
  <li>논문 publish 이상으로 궁극적으로 세상에 영향을 줄 수 있는(!) 기술을 만드는 것을 목표로 합니다.</li>
  <li>또한 top-tier 학회 및 저널에 accept되는지 보다 우리가 만족할만한 수준의 연구를 하는 것을 목표로 합니다. 그렇지만 논문 준비를 어떻게 해야하는지에 대해서도 아주 구체적으로 지도하고 있습니다.</li>
  <li>전세계의 연구자들과 교류하고 공동연구를 활발히 진행하는 것을 목표로 합니다. 연구실 내 여러 프로젝트들이 이미 해외 연구자들과의 공동 연구를 통해 진행되고 있습니다.</li>
  <li>해외 대학, 회사 및 연구소에서 학생 인턴쉽도 적극 장려합니다. 연구실 내 우수한 학생들의 해외 인턴쉽을 적극적으로 지원할 예정입니다.</li>
</ul>

<hr />
<h2 id="연구실-opening">연구실 Opening</h2>

<ul>
  <li>매해 상황이 달라질 수 있지만, 현재 ME 석사 국비 4명, 석박통합 국비 2명 TO가 있습니다. 그 외 과기원 및 산학 장학생 TO 활용은 예외적인 상황이므로 GIST ME 국비로 지원하는 것이 일반적인 방법입니다.</li>
  <li>후기 입시 때는 전기에 소진하고 남은 TO를 활용하기 때문에, 전기 입시 때 지원하시는 것을 추천합니다. 후기에 반드시 지원해야 하는 경우에는 그 전년도 여름 (8월)까지는 미리 사전 컨택을 하셔야 합니다.</li>
  <li>연구실 국비 TO가 남는다고 해서 지원자들의 저희 연구실 입학이 무조건 보장되지 않습니다.</li>
</ul>

<hr />
<h2 id="연구실-선발-기준">연구실 선발 기준</h2>

<ul>
  <li>저희 연구실에서 학부 인턴쉽을 통해 좋은 성과를 보여준 학생들을 우선적으로 고려합니다. 그렇지만 인턴쉽을 한 학생들을 무조건 선발하지는 않습니다.</li>
  <li>그 다음으로 제 수업에서 좋은 성적을 보여준 학생들을 우선적으로 고려합니다.</li>
  <li>특히 아래와 같은 부분들을 중요하게 고려합니다. (* 갯수: 중요도)
    <ul>
      <li>Motivation (****): 우리 연구실에서 구체적으로 어떤 연구를 하고 싶은지.</li>
      <li>Knowledge (***): 아래 관련 수업 수강 여부 및 성적. 또는 관련 전공 지식 수준.
        <ul>
          <li>Math: Linear Algebra, Nonliear Optimization, Filtering Theory, Probability and Statistics, etc.</li>
          <li>Others: Robotics - SO(3)/SE(3), Computer Vision, Deep Learning, Multiple View Geometry, etc.</li>
        </ul>
      </li>
      <li>GPA (***): 전체 학부 학점 및 관련 전공 과목 성적.</li>
      <li>Programming Skills (***): Python (PyTorch or Tensorflow), MATLAB, C/C++ 등 관련 구현 경험 여부.</li>
      <li>English Skills (**): 연구실 Weekly Meeting은 영어로 진행되며, 일반적으로 본인의 연구 성과는 영어로 작성된 논문으로 출판되고, 이를 영어로 학회에서 발표합니다.</li>
      <li>Experience (**): 관련 연구 및 프로젝트 참여 경험 여부. 저희 연구실에서 인턴쉽을 한 경우 큰 도움이 될 수 있습니다.</li>
    </ul>
  </li>
</ul>

<hr />
<h2 id="연구실-선발-과정">연구실 선발 과정</h2>

<ul>
  <li>지원 동기, 성적표, CV 등을 검토 후, 인터뷰 및 간단한 테스트를 거쳐 선발합니다.</li>
  <li>[지원방법] 아래 질문에 대한 답과 CV, 그리고 성적표를 저에게 이메일로 보내시면 됩니다.
    <ul>
      <li>석사 과정에 진학하려는 이유.</li>
      <li>석사 과정 이후 진로 계획.</li>
      <li>우리 연구실에 지원하는 이유.</li>
      <li>관심 연구 분야 및 관련 경험.</li>
    </ul>
  </li>
  <li>연구실 입학에 관심 있으신 분들은 아래 학부 인턴쉽을 먼저 지원하시길 강력히 권장합니다.</li>
</ul>

<hr />
<h2 id="연구실-인턴쉽">연구실 인턴쉽</h2>

<ul>
  <li>매년 여름/겨울 방학마다 인턴을 모집하고 있습니다. 매 방학마다 2-3명 정도의 인턴을 모집했습니다.</li>
  <li>여름/겨울 방학 8주 기간 동안 full-time으로 연구실에 있을 수 있는 분들을 리크루팅하고 있습니다.</li>
  <li>모든 인턴 분들은 대학원생들과 마찬가지로 개별 연구 프로젝트를 진행하는 것을 목표로 합니다.</li>
  <li>진행하는 연구 프로젝트 및 주제에 따라 stipend가 지급될 수도 있습니다.</li>
  <li>방학 이후에도 프로젝트를 이어서 진행할 수 있는 분들에게 우선권을 드리고 있습니다.</li>
  <li>참고로 다음과 같은 학부 인턴을 우선적으로 선발합니다.
    <ul>
      <li>선형대수, 비선형 최적화, 입문 수준의 프로그래밍 등 학부 기초 지식이 튼튼한 학생.</li>
      <li>프로그래밍 언어를 하나 이상 깊게 다루어본 학생 (Python, MATLAB, C/C++, Swift 등 무관).</li>
      <li>지적 호기심을 가지고 새로운 분야를 탐구하기 원하는 학생.</li>
      <li>도전적인 목표를 세우며 꾸준히 성장하는 성실한 학생.</li>
      <li>설정한 목표는 어김없이 성취하며 끝마무리를 잘 맺는 학생.</li>
    </ul>
  </li>
</ul>

<hr />
<h2 id="연구실-faq">연구실 FAQ</h2>

<ul>
  <li>출퇴근 시간: 보통의 일과 시간 (오전 10시–오후 6시)에는 연구실에 있는 것을 원칙으로 하고 있지만, 출퇴근을 특별히 체크하지 않습니다. 다만, 연구실에서 자주 볼 수 없는 경우에는 시간을 지정할 수 있습니다. 수업/미팅 외에 개인 용무로 반나절 이상 자리를 비우는 경우 미리 알려주시길 부탁드리고 있습니다.</li>
  <li>연구실 미팅: 매주 돌아가면서 본인이 수행중인 연구에 대한 Progress에 대해 발표를 진행합니다. Weekly Meetings은 외국인 학생이 없어도 영어로 진행됩니다. 모든 연구실 구성원들이 참여해야 합니다.</li>
  <li>인턴쉽 관련: 타대 학생도 지원 가능하며, 외부 학생들도 방학 중에는 학교 기숙사를 이용할 수 있고, 인턴 기간 중 학교 시설을 이용하는 게 가능합니다. 또한 학기 중에는 새로 인턴을 채용하지 않습니다.</li>
</ul>

<!-- 
이 글은 제 연구 지도 계획을 다루고 있습니다.
저와 함께 진행하는 연구를 통해 MPIL 석사/박사과정 학생들이 *실력있고 창의적인 연구자*로 성장하기를 바랍니다.
더불어 저도 좋은 멘토, 연구자로 성장하기 위해 끊임 없이 노력할 계획입니다.

대학원 과정은 학사 과정과는 다르게 기존 지식 습득보다는 *새로운 지식 창조*를 강조합니다.
따라서, 대학원생들은 학위 과정 동안 치열한 연구를 통해 기존 기술이 지닌 한계점을 개선/극복하고 인류 기술 발전 과정에 참여합니다.
처음에는 이 과정이 매우 어렵고 힘들 수 밖에 없습니다.
지금까지 익숙하게 해오던 학습 방식에서 벗어나, 비판적으로 사고하며 창의적으로 문제를 해결해야 하기 때문입니다.
더불어 연구는 교과 시험과는 다르게 언제 연구 성과가 도출될지, 열심히 하면 결과가 나오기는 하는 것인지 알 수 없어 엄청난 노력에도 불구하고 시련을 경험하기도 합니다. 
저 역시 학위 과정을 처음 시작하며 다양한 어려움을 겪었고, 지도 교수님과 여러 선후배들의 도움으로 박사 과정을 무사히 마칠 수 있었습니다.
저는 체계적이고 일관적인 지도를 통해 석사/박사 과정 학생들이 연구 수행 과정에서 *불필요한 시행착오*를 최대한 줄이고 성공적으로 학위 과정을 마칠 수 있도록 도울 예정입니다.

또한, 대학원 신입생들은 대개 학부 과정을 마치고 연구실 생활을 통해 조직/단체 생활을 처음 접합니다.
그렇기 때문에 처음에는 연구실 생활이 낯설고, 다른 사람과 협업하는 일이 어색할 수 있습니다.
하지만, 학생들은 점진적으로 연구 경험을 쌓으면서 연구 수행 능력을 기를 것입니다.
그리고 전문가는 단순히 개인의 실력만 뛰어난 사람이 아니라 효과적인 협업과 소통을 통해 1+1=3을 달성하는 사람이라는 사실을 배울 것입니다.
저는 MPIL 석사/박사 과정 학생들이 연구실 생활을 거치면서 **같이 일하고 싶은 인재** 뿐 아니라 **같이 실패해도 괜찮은 사람**으로 성장하기를 바랍니다.


---
## 원칙
다음은 연구 지도 과정에서 제가 지키고자 하는 세 가지 원칙입니다.

```
1. 자율
2. 책임
3. 보상
```

<ins>자율</ins>: 자율성은 창의적으로 사고할 수 있는 전제 조건이라고 생각합니다.
그렇기 때문에 연구 주제 선정부터 출퇴근 시간까지 최대한 자율적인 연구 환경을 조성하고자 합니다.
하지만, 연구 수행에 있어 처음부터 너무 큰 자율성이 주어지는 경우 오히려 역효과가 날 수 있습니다.
스스로 결정하고, 창의적으로 사고하는 과정이 익숙하지 않을 수 있기 때문입니다.
따라서 연구 수행 초기에는 개개인의 특성과 상황에 맞게, 세세한 지도가 필요할 수도 있습니다.

<ins>책임</ins>: 책임이 없는 자율은 자칫 방종과 방치로 이어질 수 있습니다.
이러한 부작용을 막기 위해, 각 연구실 구성원들은 본인이 설정한 연구 목표와 연구 계획을 세미나를 통해 연구실의 다른 구성원들과 공유합니다.
더불어 본인이 수행한 연구 내용을 다른 구성원들과 공유하면서 자연스럽게 책임감을 느낄 수 있습니다.

<ins>보상</ins>: 보상은 우리에게 왕성하게 연구를 수행할 동기를 부여하기도 하고, 어려운 상황에서 삶의 활력소가 되기도 합니다.
대학원 생활에서 얻을 수 있는 가장 큰 보상은 학위 수여와 논문 출판이지만, 아쉽게도 두 보상은 준비 시간이 오래 걸립니다.
저는 학생들이 작은 성공 (보상)을 자주 경험함으로써 학위 기간을 활기차게 그리고 능동적으로 보낼 수 있도록 돕고자 합니다.
여기에서 말하는 보상은 물질적 보상뿐 아니라 해외 우수 학회 참석, 해외 연수, 공로 표창 등의 보상을 포함합니다.


---
## 문화
다음은 효과적인 연구 수행을 위해 연구실에서 추구하는 두 가지 문화입니다.

```
1. 공유
2. 소통
```

<ins>공유</ins>: 공유는 연구실 내에서 진행하는 연구 프로젝트 내용과 각자의 기여도를 모든 구성원들이 서로 확인할 수 있는 상태를 의미합니다.
이렇게 개인의 연구 진행 상황을 다른 동료들과 실시간으로 공유하는 데에는 큰 용기가 필요합니다.
공유를 통해 자신의 약점과 논리의 허점이 그대로 노출되기 때문입니다.
하지만 공유는 투명성을 이끌어냄으로써 전문성을 기를 수 있는 최적의 환경을 제공합니다.
인공지능 분야가 짧은 기간 동안 급속하게 발전한 배경에도 이러한 공유가 있었습니다.
그리고 공유를 통해 진솔한 소통의 장이 마련됩니다.

<ins>소통</ins>: 진솔한 소통은 사람 간의 돈독한 관계를 만들어주기도 하고, 연구 환경에서는 깊이 있는 연구를 가능케 합니다.
한 사람이 생각한 연구 아이디어나 해석보다는 여러 사람이 함께 다양한 시각으로 바라본 분석이 훨씬 깊이 있기 때문입니다.
원활한 소통을 위해서는 상대방 입장에서 생각하는 역지사지 자세가 필요합니다.
다시 말하면, 내가 알고 있는 것을 상대는 모를 수도 있다는 전제로 대화를 이어가면 좀 더 성숙하게 대화할 수 있습니다.
또한, 감정적인 어휘 대신 구체적인 근거/수치를 통해 이야기를 풀어가면 서로의 감정을 상하지 않고 대화할 수 있습니다.
예를 들어, "이 연구 아이디어는 되게 별로야"보다는 "이 연구 아이디어는 A논문과 B논문을 결합한 형태라 novelty 공격을 받을 수 있을 것 같아"라고, "이 실험 결과는 가치가 없어"보다는 "이전 논문들은 최고 성능을 5% 정도 향상시켰는데 이 결과는 1% 향상이라 실험이 더 필요할 것 같아"라고 말하면 좀 더 성숙하게 대화할 수 있습니다.

---
## 역량
다음은 과정별로 기르고자 하는 역량입니다.
> 석사 과정
  - ###### 선행 연구 논문을 이해하고 이를 비판적으로 검토할 수 있음.
  - ###### 선행 연구의 한계점을 부분적으로 파악하고 한계점 해결을 위한 대안을 제시할 수 있음.
  - ###### 연구 프로젝트<sup>[1]</sup>에 참여하여 각 단계에 기여할 수 있음.


> 박사 과정
  - ###### 관련 분야의 흐름과 최신 연구 동향을 세세하게 파악하고 있음.
  - ###### 연구 문제를 정의하고 창의적으로 해결할 수 있음.
  - ###### 독립적으로 연구 프로젝트를 구성하고 관리할 수 있음.

<!-- 
다음은 연차별로 기르고자 하는 역량입니다. 물론, 본인 역량이 아래에 언급 되어 있는 역량을 상회할 수 있고 시작점이 다를 수 있기 때문에 절대적이지는 않습니다. 그리고 한 가지 주의할 점은 연차는 말 그대로 역량을 의미할 뿐 높은 위계(rank)를 의미하지는 않습니다.

<table style="width:100%">
<tr style="border-top: 2px solid black; border-bottom: 1px solid black;">
<th width="20%" style="text-align: center">
<strong>연차</strong>
</th>
<th width="40%" style="text-align: center">
<strong>연구 역량</strong>
</th>
<th width="40%" style="text-align: center">
<strong>협업 역량</strong>
</th>
</tr>

<tr>
<td style="text-align: center">
석사 1년차
</td>
<td>
관련 선행 연구 논문을 이해하고 설명할 수 있음 <br/>
연구를 수행하기 위한 전공 지식을 지니고 있음
</td>
<td>
주어지는 사양의 시스템을 구현할 수 있음
</td>
</tr>

<tr style="border-bottom: 1px solid black;">
<td style="text-align: center;">
석사 2년차
</td>
<td>
선행 연구의 한계점을 부분적으로 파악할 수 있음 <br/>
한계점 해결을 위한 아이디어를 제시할 수 있음
</td>
<td>
연구 프로젝트<sup>[1]</sup>에 참여하여 각 단계에 기여할 수 있음
</td>
</tr>

<tr>
<td style="text-align: center">
박사 1년차
</td>
<td>
도움을 받아 연구 문제를 정의할 수 있음 <br/>
도움을 받아 연구 문제를 해결할 수 있음
</td>
<td>
소규모 연구 프로젝트를 구성하고 완수할 수 있음
</td>
</tr>

<tr>
<td style="text-align: center">
박사 2년차
</td>
<td>
한 분야의 흐름과 최신 연구 동향을 파악하고 있음 <br/>
조금 도움을 받아 연구 문제를 정의하고 해결할 수 있음
</td>
<td>
연구 프로젝트를 구성하고 관리할 수 있음
</td>
</tr>

<tr>
<td style="text-align: center">
박사 3년차
</td>
<td>
관련 분야의 흐름과 최신 연구 동향을 파악하고 있음 <br/>
연구 문제를 정의하고 해결할 수 있음
</td>
<td>
여러 연구 프로젝트를 관리할 수 있음
</td>
</tr>

<tr style="border-bottom: 2px solid black;">
<td style="text-align: center">
박사 4년차
</td>
<td>
다수 분야의 흐름과 최신 연구 동향을 파악하고 있음 <br/>
독립적으로 연구를 수행할 수 있음
</td>
<td>
여러 연구 프로젝트를 구성하고 관리할 수 있음
</td>
</tr>

</table>
-->
<p><br />
<!-- 
<sup> [1] 연구 프로젝트: 선행연구 조사, 한계점 도출, 한계점 해결을 위한 기여점(contribution) 제시, 타당성 검증, 실험 설계, 실험 수행, 실험 결과 분석, 논문 작성, 제출, 퇴고 등 연구 성과 도출을 위한 일련의 과정 </sup>
--></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction to MPIL for Graduate Students]]></summary></entry><entry><title type="html">Open Positions</title><link href="/blog/2023/open-positions-kor/" rel="alternate" type="text/html" title="Open Positions" /><published>2023-09-21T00:00:00+00:00</published><updated>2023-09-21T00:00:00+00:00</updated><id>/blog/2023/open-positions-kor</id><content type="html" xml:base="/blog/2023/open-positions-kor/"><![CDATA[<p>이 글은 우리 연구실 합류에 도움이 될 정보를 다루고 있습니다.
우리 연구실에 관심이 있다면 아래 글을 참고하여 저에게 이메일로 연락 바랍니다.
이 글에서 제공하는 정보 외에 더 궁금한 점을 묻는 질문은 언제든 환영합니다.</p>

<hr />

<h2 id="to-상황">TO 상황</h2>
<p>우리 연구실은 입학생 전원 국비 또는 과기원 장학생으로 선발하여 매년 3명 내외로 새로운 구성원을 모집합니다.
모집 과정은 통합/석사/박사 과정입니다.
다만, 박사 과정은 TO가 많지 않고, 석사 과정 중 연구 성과를 도출할 것으로 보이는 경우에만 선발합니다.
석사 과정 중에 연구 성과가 도출되어야 연구실에서 의미 있는 연구 경험을 쌓을 수 있고, 본인 커리어에도 도움이 되기 때문입니다.</p>

<p>*학부 연구생은 TO가 유동적입니다 (아래 참조).<br />
*박사후연구원 (postdoctoral researcher)은 CV와 함께 연락 바랍니다.</p>

<hr />

<h2 id="대학원-과정">대학원 과정</h2>
<p>대학원 진학을 생각하고 있다면 대학원 입시 최소 6개월 전에는 학부 인턴 연구원으로 지원하기를 추천합니다.
대학원 과정은 짧게는 2년, 길게는 5년 이상이 걸리기 때문에 신중하게 연구실을 선택하는 것이 중요합니다.
아래는 각 과정에 대한 설명입니다.</p>

<blockquote>
  <p>박사/통합 과정</p>
  <ul>
    <li>
      <h6 id="저와-연구-수행-경험이-있거나-신뢰할-만한-추천을-받은-학생-중에서-선발합니다">저와 연구 수행 경험이 있거나 신뢰할 만한 추천을 받은 학생 중에서 선발합니다.</h6>
    </li>
    <li>
      <h6 id="저와-연구-경험이-없거나-추천을-받지-못한-경우-학부-연구생이나-석사-과정으로-지원-바랍니다">저와 연구 경험이 없거나 추천을 받지 못한 경우, 학부 연구생이나 석사 과정으로 지원 바랍니다.</h6>
    </li>
    <li>
      <h6 id="석사-학위만-취득하기를-원하면-꼭-석사-과정으로-지원해야-하고-통합-과정은-박사-학위를-취득하기-원하는-학생에게만-to를-배정합니다">석사 학위만 취득하기를 원하면 꼭 석사 과정으로 지원해야 하고, 통합 과정은 박사 학위를 취득하기 원하는 학생에게만 TO를 배정합니다.</h6>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p>석사 과정</p>
  <ul>
    <li>
      <h6 id="석사-과정-지원-학생에-대해-심도-깊은-선행-연구-경험을-요구하지-않습니다">석사 과정 지원 학생에 대해 심도 깊은 선행 연구 경험을 요구하지 않습니다.</h6>
    </li>
    <li>
      <h6 id="하지만-의미-있는-선행-연구-경험은-강력한-이점으로-작용합니다">하지만, 의미 있는 선행 연구 경험은 강력한 이점으로 작용합니다.</h6>
    </li>
    <li>
      <h6 id="석사-과정-학생은-지원서류-검토-후-면담과-간단한-테스트를-거쳐-선발합니다">석사 과정 학생은 지원서류 검토 후, 면담과 간단한 테스트를 거쳐 선발합니다.</h6>
    </li>
    <li>
      <h6 id="지원방법-아래-질문에-대한-답과-cv-성적표를-저에게-이메일로-보내기-바랍니다">[지원방법] 아래 질문에 대한 답과 CV, 성적표를 저에게 이메일로 보내기 바랍니다.</h6>
      <ul>
        <li>
          <h6 id="석사-과정에-진학하려는-이유-한국어-혹은-영어">석사 과정에 진학하려는 이유 (한국어 혹은 영어)</h6>
        </li>
        <li>
          <h6 id="석사-과정-이후-진로-계획-한국어-혹은-영어">석사 과정 이후 진로 계획 (한국어 혹은 영어)</h6>
        </li>
        <li>
          <h6 id="우리-연구실에-지원하는-이유-한국어-혹은-영어">우리 연구실에 지원하는 이유 (한국어 혹은 영어)</h6>
        </li>
        <li>
          <h6 id="관심-연구-분야-및-관련-경험-한국어-혹은-영어">관심 연구 분야 및 관련 경험 (한국어 혹은 영어)</h6>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr />

<h2 id="학부-연구생">학부 연구생</h2>
<p>학부 인턴 연구원은 매 학기와 방학 시작 전 모집하여 학사 분기 단위 (1-2월/3-6월/7-8월/9-12월)로 운영합니다.
학부 인턴은 최소 6개월을 추천합니다.
더불어, 우리 연구실에서는 인턴 연구와 졸업 논문, G-SURF에 대하여 비슷한 연구 성과를 기대하고 있습니다 (인턴과 졸업 연구 병행 가능).
인턴 연구원에 관심이 있다면, 학사 분기 시작 2주 전까지 지원하기 바랍니다.
지원시 성적표와 본인을 드러낼 수 있는 내용을 포함하기 바랍니다.
참고로 다음과 같은 인턴 연구원을 우선적으로 선발합니다.</p>
<ul>
  <li>선형대수, 비선형 최적화, 입문 수준의 프로그래밍 등 학부 기초 지식이 튼튼한 학생</li>
  <li>프로그래밍 언어를 하나 이상 다루어본 학생 (Python, MATLAB, C/C++, Swift 등 무관)</li>
  <li>지적 호기심을 가지고 새로운 분야를 탐구하기 원하는 학생</li>
  <li>도전적인 목표를 세우며 꾸준히 성장하는 성실한 학생</li>
  <li>설정한 목표는 어김없이 성취하며 끝마무리를 잘 맺는 학생</li>
</ul>

<p>아래는 학부 인턴 연구원 운영 방식입니다.</p>

<blockquote>
  <p>스터디 인턴</p>
  <ul>
    <li>
      <h6 id="스터디를-통해-연구-수행을-위한-기본-지식을-학습합니다">스터디를 통해 연구 수행을 위한 기본 지식을 학습합니다.</h6>
    </li>
    <li>
      <h6 id="매주-주어진-내용을-학습하고-과제를-수행한-후-정기-미팅을-통해-질의응답을-진행합니다">매주 주어진 내용을 학습하고, 과제를 수행한 후 정기 미팅을 통해 질의응답을 진행합니다.</h6>
    </li>
    <li>
      <h6 id="스터디-인턴에게는-연구-및-공부를-위한-개인-연구-공간-개인-데스크탑이-지급됩니다">스터디 인턴에게는 연구 및 공부를 위한 개인 연구 공간, 개인 데스크탑이 지급됩니다.</h6>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p>연구 인턴</p>
  <ul>
    <li>
      <h6 id="연구-인턴은-연구-과제에-참여하여-실질적인-연구를-수행합니다">연구 인턴은 연구 과제에 참여하여 실질적인 연구를 수행합니다.</h6>
    </li>
    <li>
      <h6 id="대개-한두명의-대학원생과-함께-연구를-진행하고-저와-정기적인-연구-회의를-가집니다">대개 한두명의 대학원생과 함께 연구를 진행하고, 저와 정기적인 연구 회의를 가집니다.</h6>
    </li>
    <li>
      <h6 id="연구-인턴은-스터디-과정을-마쳤거나-그에-준하는-배경-지식을-갖추어야-참여-가능합니다">연구 인턴은 스터디 과정을 마쳤거나 그에 준하는 배경 지식을 갖추어야 참여 가능합니다.</h6>
    </li>
    <li>
      <h6 id="연구-프로젝트에-참여하는-연구-인턴에게는-학생인건비-및-개인-랩탑이-지급됩니다">연구 프로젝트에 참여하는 연구 인턴에게는 학생인건비 및 개인 랩탑이 지급됩니다.</h6>
    </li>
  </ul>
</blockquote>

<!--

---

## GIST 학부생을 위한 부언
대학원 생활은 연구 성과가 바로바로 눈에 띄지 않기 때문에 무너지기 쉽고 좌절하기 [](https://gradschoolstory.net/)

```
1. 호기심
2. 목표/비전
```

<ins>호기심</ins>:

<ins>목표/비전</ins>:
<br/>
<br/>



---

## 지원 방법
아래 질문에 대한 답과 CV를 저에게 이메일로 보내기 바랍니다.
- 10년 뒤 본인이 그리는 미래(비전)
- 대학원에 진학하는 이유(혹은 인턴 목적)
- 가장 자랑스러운 성취 경험
- 실패 경험(없을 경우 답하지 않아도 무방)
- 본인의 강점과 발전시키고 싶은 부분
<br/>
<br/>
-->]]></content><author><name></name></author><summary type="html"><![CDATA[Information for Joining MPIL at GIST]]></summary></entry><entry><title type="html">Research Statement</title><link href="/blog/2021/research-eng/" rel="alternate" type="text/html" title="Research Statement" /><published>2021-10-01T00:00:00+00:00</published><updated>2021-10-01T00:00:00+00:00</updated><id>/blog/2021/research-eng</id><content type="html" xml:base="/blog/2021/research-eng/"><![CDATA[<p>We design and build autonomous systems that process visual and sensor data <strong>to realize meaningful services for humans</strong>. With an unprecedented scale of available data and computational resources, computer vision and robotics systems could better understand the surrounding environments and improve user experience in human-robot interaction (HRI) scenarios. In various domains, our research aims to design autonomous systems that 1) extract both semantic and physical information from visual data for understanding the surrounding environments and 2) elicit user behavior, intention or message from sensor data for promoting natural HRI experiences. Our research attempts to take a step toward the long-dreamed goal of replacing repetitive and dangerous tasks with autonomous systems and natural interaction between human and autonomous systems.</p>

<p>Specifically, our research has focused on making autonomous systems better understand two crucial components of <strong>robot-in-the-loop research scenarios</strong>:</p>

<ul>
  <li>the surrounding environments and</li>
  <li>recognition of human behavior, intention, or message.</li>
</ul>

<p>First of all, autonomous systems must be able to perceive the semantic entities inherent in the environments as well as the geometry of the environments in order to perform high-level tasks such as errands, cleaning, cooking, and answering questions. If the systems collect just one type of information among the physical information and semantics of the surrounding environment, they can merely perform simple tasks and cannot provide meaningful services to humans.</p>

<p>Next, recognizing human behavior, intention or message in an efficient and effective manner is the very starting point of providing appropriate services to humans. Failing to recognize this information results in irrelevant services—leading to user dissatisfaction. autonomous systems gather information regarding human behavior, intention, or message through various sensor devices; computationally process the information; and detect human behavior, intention, or message.</p>

<hr />

<h2 id="visual-understanding-for-embodied-ai">Visual Understanding for Embodied AI</h2>
<h3 id="simvodis-simultaneous-visual-odometry-object-detection-and-instance-segmentation1">SimVODIS: Simultaneous Visual Odometry, Object Detection and Instance Segmentation<sup>[1]</sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_simvodis.png" alt="SimVODIS Overview" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 1. Overview of the proposed SimVODIS. SimVODIS receives a set of three consecutive images and then estimates semantics (object classes, bounding boxes and object masks), relative poses between input images, and the depth map of the center image.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a computationally-efficient network architecture to recognize both semantic and geometric information of the surrounding environments. <br />
<strong>[Limitations of Convention]</strong> Conventional approaches combine two techniques to associate geometric and semantic information elicited from the surrounding environments. However, running both algorithms simultaneously requires a lot of computation resources and complicates the software structure. <br />
<strong>[Contribution]</strong> First, we defined a fully data-driven semantic VO algorithm, SimVODIS, for the first time. We expect SimVODIS would provoke the evolution of data-driven VO towards semantic VO/SLAM. Second, we designed the SimVODIS network which simultaneously performs both geometric and semantic tasks. The network conducts multiple tasks utilizing shared feature maps and runs in one thread. Third, we made the source code of the proposed SimVODIS network and the pretrained network parameters open-source.</p>

<h3 id="3-d-scene-graph-a-sparse-and-semantic-representation-of-physical-environments-for-intelligent-agents2">3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents<sup>[2]</sup></h3>
<table class="demo_tb" style="width:50%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<iframe style="width:100%; height:300px;" src="https://www.youtube.com/embed/GXNQAMYU-yQ">
</iframe>
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design an accurate, applicable, usable and scalable environment model for intelligent agents to store observed and perceived environmental information. <br />
<strong>[Limitations of Convention]</strong> Raw and dense representations representing environments with minimum distortion require massive memory space, take much time for processing and lack any semantic information. Moreover, the coverage of abstractive and descriptive representations is confined to the field of view (FoV) of the cameras and the output is in the form of natural language, which makes it difficult for other AI applications to utilize. <br />
<strong>[Contribution]</strong> First, we defined the concept of the 3-D scene graph which represents the environments in an accurate, applicable, usable, and scalable way. Second, we designed the 3-D scene graph construction framework which generates 3-D scene graphs for environments upon receiving a sequence of observations. Third, we provided two application examples of the 3-D scene graph: a) VQA and b) task planning. Fourth, we made the source code of the algorithms open-source.</p>

<h3 id="dual-task-learning-by-leveraging-both-dense-correspondence-and-mis-correspondence-for-robust-change-detection-with-imperfect-matches3">Dual Task Learning by Leveraging Both Dense Correspondence and Mis-Correspondence for Robust Change Detection With Imperfect Matches<sup>[3]</sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_simsac.gif" alt="SimSaC Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 2. The proposed SimSaC displays robust performance even given imperfect matches of reference and query images with which conventional methods fail.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a robust scene change detection algorithm given imperfect mathches which are more plausible in the real-world scenarios. <br />
<strong>[Limitations of Convention]</strong> Contemporary approaches for SCD, however, assumes an ideal match of the scene between the current and
the past time steps although observing the same scene with a perfect match hardly occurs in real-world applications. Thus, contemporary approaches would not display the reported performance when deployed to practical systems. <br />
<strong>[Contribution]</strong> First, we carefully formulated a change detection task that reflects performance in realworld settings for the first time. Second, we proposed the SimSaC network for robust change detection which leverages both dense correspondence (scene flow) and miscorrespondence (change). Third, we designed a training scheme that enhances the robustness of change detection without requiring additional annotations. Fourth, we collected a new benchmark dataset consisting of imperfect matches for measuring change detection performance in real-world scenarios. Fifth, we made the source code of the algorithms open-source.</p>

<hr />

<h2 id="recognition-of-human-behavior-intention-or-message">Recognition of Human Behavior, Intention or Message</h2>
<h3 id="a-stabilized-feedback-episodic-memory-sf-em-and-home-service-provision-framework-for-robot-and-iot-collaboration4">A Stabilized Feedback Episodic Memory (SF-EM) and Home Service Provision Framework for Robot and IoT Collaboration<sup>[4]<sup></sup></sup></h3>
<table class="demo_tb" style="width:50%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<iframe style="width:100%; height:300px;" src="https://www.youtube.com/embed/7ES-n5MLqmY">
</iframe>
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a computational episodic memory that learns human behaviors and reasons human intentions. <br />
<strong>[Limitations of Convention]</strong> Conventional learning and reasoning algorithms such as hidden Markov model (HMM) or reinforcement learning (RL) either learn in an off-line setting or suffer from the curse of dimensionality. Adaptive theory resonance (ART) networks display instability in long-term scenarios and lack a feedback mechanism. <br />
<strong>[Contribution]</strong> First, we designed a stabilized memory system with a feedback mechanism for incremental learning of human behaviors and reasoning human intentions. Second, we proposed a home service provision framework for robot and IoT collaboration using the proposed SF-ART architecture. Third, we set up a Smart Home environment and verify the effectiveness of both the proposed memory architecture and the service framework.</p>

<h3 id="i-keyboard-fully-imaginary-keyboard-on-touch-devices-empowered-by-deep-neural-decoder5">I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep Neural Decoder<sup>[5]<sup></sup></sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_I-Keyboard.gif" alt="I-Keyboard Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 3. Users could type on an empty touch screen with ten fingers imagining a keyboard layout. Since users could follow their own typing habit, the usability enhances.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a natural text-entry method for humans to deliver messages through touch screens (users can start typing using ten fingers on any position at any angle on touch screens without worrying about the keyboard position and shape). <br />
<strong>[Limitations of Convention]</strong> Soft keyboards’ lack of tactile feedback increases the rate of typos and soft keyboards hinder mobile devices from presenting enough content because they occupy a relatively large portion on displays. <br />
<strong>[Contribution]</strong> First, we defined an advanced typing scenario where both a predefined typing area and a calibration step are omitted. Second, we collected user data in an unconstrained environment and comprehensively analyzed user behaviors in such an environment. Third, we designed a deep learning architecture for a shape, position, and angle-independent decoding and formulated the auxiliary loss for training the architecture.</p>

<h3 id="type-anywhere-you-want-an-introduction-to-invisible-mobile-keyboard6">Type Anywhere You Want: An Introduction to Invisible Mobile Keyboard<sup>[6]<sup></sup></sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_IMK.gif" alt="IMK Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 4. IMK decodes both current and past inputs. As an user writes more text, IMK utilizes more context and the accuracy improves.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a natural text-entry method for humans to deliver messages through mobile touch screens. <br />
<strong>[Limitations of Convention]</strong> The limitations of soft keyboards for mobile devices include high rate of typos, lack of tactile feedback, inconsideration of users’ different keyboard mental model, and large screen occupation. <br />
<strong>[Contribution]</strong> First, we collected a large-scale, richly annotated IMK dataset which contains approximately 2M pairs of touch points and text. Second, we analyzed the characteristics of user behavior on the new proposed task in-depth. Third, we designed a novel deep neural architecture, Self-Attention Neural Character Decoder (SA-NCD) as a baseline decoder.</p>

<div id="line_breaker" style="clear: both;">
<br />
</div>

<h3 id="writing-in-the-air-unconstrained-text-recognition-from-finger-movement-using-spatio-temporal-convolution7">Writing in The Air: Unconstrained Text Recognition from Finger Movement Using Spatio-Temporal Convolution<sup>[7]<sup></sup></sup></h3>

<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_WiTA.gif" alt="WiTA Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 5. An example instance of the dataset collected in this work. The person in the example is writing "re" from the word "recognized".
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a computational model that interprets the text written in the air with the index finger using an RGB camera. <br />
<strong>[Limitations of Convention]</strong> Conventional WiTA systems hardly achieve satisfactory performance for practical deployment into real-world applications. Moreover, there is no available benchmark dataset suitable for development and evaluation of WiTA systems. <br />
<strong>[Contribution]</strong> First, we explicitly defined the task of WiTA and collect five types of the dataset in two languages (Korean and English) for the development and evaluation of WiTA systems. Second, we designed baseline methods for the WiTA task and proposed spatio-temporal residual network architectures which translate an input video sequence into a sequence of characters written in the air. Third, we  made the data collection tool, the WiTA dataset, the proposed WiTA baseline networks and the pretrained network parameters open-source.</p>

<hr />

<p><sup> [1] <strong>U.-H. Kim</strong>, S.-H. Kim and J.-H Kim, “SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance Segmentation,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 44, no. 1, pp. 428-441, Jan. 2022. [SCIE, IF: 16.390] </sup><br />
<sup> [2] <strong>U.-H. Kim</strong>, J.-M. Park, T.-J. Song and J.-H Kim, “3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents,” IEEE Trans. on Cybernetics, vol. 50, no. 12, pp. 4921-4933, Dec. 2020. [SCIE, IF: 11.079] </sup><br />
<sup> [3] J.-M. Park*, <strong>U.-H. Kim*</strong>, S.-H. Lee and J.-H Kim, “Dual Task Learning by Leveraging Both Dense Correspondence and Mis-Correspondence for Robust Change Detection With Imperfect Matches,” IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13749-13759, Jun. 2022 [Oral Presentation] </sup><br />
<sup> [4] <strong>U.-H. Kim</strong> and J.-H Kim, “A Stabilized Feedback Episodic Memory (SF-EM) and Home Service Provision Framework for Robot and IoT Collaboration,” IEEE Trans. on Cybernetics, vol.  50, no.5, pp.  2110-2123, May 2020. [SCIE, IF: 11.079] </sup><br />
<sup> [5] <strong>U.-H. Kim</strong>, S.-M. Yoo and J.-H Kim, “I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep Neural Decoder,” IEEE Trans. on Cybernetics, Early Access, Sep. 2019. [SCIE, IF: 11.079] </sup><br />
<sup> [6] S.-M. Yoo*, <strong>U.-H. Kim*</strong>, Y.-W. Hwang and J.-H. Kim (* equal contribution), “Type Anywhere You Want: An Introduction to Invisible Mobile Keyboard,” in Proceedings of the 30-th International Joint Conference on Artificial Intelligence (IJCAI), 2021. (13.9% acceptance rate) </sup><br />
<sup> [7] <strong>U.-H. Kim*</strong>, Y.-W. Hwang*, S.-K. Lee and J.-H. Kim (* equal contribution), “Writing in The Air: Unconstrained Text Recognition from Finger Movement Using Spatio-Temporal Convolution,” arXiv, 2104.09021, 2021. </sup><br /></p>

<script>
  if ((window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth) < 600) {
    document.getElementById("line_breaker").innerHTML = "";
    var tables = document.getElementsByClassName("demo_tb");
    for( var i = 0; i < tables.length; i++ ){
        var single_table = tables.item(i);
        single_table.style.width = "100%";
        single_table.style.margin = "0px; 0px; 0px; 0px";
    }
  }
</script>]]></content><author><name></name></author><summary type="html"><![CDATA[Intelligent Interaction & Semantic and Geometric Understanding of Environments]]></summary></entry></feed>